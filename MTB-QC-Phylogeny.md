
## ğŸ§ª MTB Genome Analysis of XDR-TB from Ethiopia

This workflow performs **high-confidence genomic analysis** of **extensively drug-resistant Mycobacterium tuberculosis (XDR-TB)** strains from Ethiopia, combining raw data QC, variant calling, and phylogenetic inference.  

### 1ï¸âƒ£ Explore Raw FASTQ Files
- Inspect **paired-end reads**: count, read length distribution, and base composition (A/T/G/C).  
- Balanced bases âœ… â†’ good-quality sequencing data.

### 2ï¸âƒ£ Quality Control & Trimming with **FASTP**
- Remove adapters, trim low-quality bases, and filter short reads.  
- Evaluate **pre- vs post-trimming metrics** to measure effectiveness.  
- Generate **per-sample HTML & JSON QC reports**.

### 3ï¸âƒ£ Aggregate QC Reports with **MultiQC**
- Combine all FASTQ and FASTP QC reports into a **single interactive HTML report**.  
- Quickly identify samples with potential issues.

### 4ï¸âƒ£ Drug Resistance Screening with **TB-Profiler**
- Check **raw FASTQ quality** and remove extremely low-quality samples.  
- Generate initial **drug-resistance predictions** per isolate.

### 5ï¸âƒ£ Variant Calling with **Snippy**
- Align reads to **H37Rv reference genome**.  
- Produce **BAM files** and preliminary **VCFs**.

### 6ï¸âƒ£ BAM Quality Check with **Qualimap**
- Assess **coverage, mapping quality, and read distribution** of BAM files.  
- Aggregate all **Qualimap QC reports** into a **single summary** to easily review per-sample metrics.  
- Ensure reliability for downstream variant filtering.

### 7ï¸âƒ£ High-Confidence Variant Filtering with **tb_variant_filter**
- Mask **Refined Low Confidence (RLC) regions**.  
- Retain **only high-confidence variants** for accurate analysis.

### 8ï¸âƒ£ Consensus Genome Generation with **BCFtools** & Outgroup Inclusion
- Generate **per-sample consensus FASTA sequences** using **BCFtools**, incorporating only high-confidence variants.  
- Include **SRR10828835 as an outgroup** to root the phylogenetic tree and provide directionality for evolutionary analysis.

### 9ï¸âƒ£ Multiple Sequence Alignment using **MAFFT** 
- Align all consensus sequences using **MAFFT** for consistent comparison.  

### ğŸ”Ÿ Phylogenetic Tree Construction with **IQ-TREE**
- Build **maximum-likelihood trees** with **ultrafast bootstrap (1000 replicates)**.  
- Visualize with **TB-Profiler ITOL outputs** to include resistance and metadata.

---

## ğŸ›  Tool Installation (Separate Conda Environments)
We use separate Conda environments for each tool to ensure reproducibility, avoid software conflicts, and simplify maintenance. Different tools may require different versions of Python or libraries, and installing them together can cause one tool to break when another requires a different version. By isolating each tool in its own environment, we can safely update or reinstall software without affecting the rest of the pipeline, keep the base system clean, and allow other researchers to reproduce the exact setup by exporting and sharing environment files. This approach ensures a robust, conflict-free, and easily maintainable MTB WGS workflow.

### Prerequisites
```bash
conda config --add channels defaults
conda config --add channels bioconda
conda config --add channels conda-forge
```
Every time we install tools in a new environment, we deactivate the conda 
```bash
conda deactivate 
```

FASTP
```bash
conda create -n fastp_env fastp=0.23.3 -y
conda activate fastp_env
fastp --version
```

MultiQC
```bash
conda create -n multiqc_env multiqc=1.16 -y
conda activate multiqc_env
multiqc --version
```
TB-Profiler
```bash
conda deactivate
conda install -n base -c conda-forge mamba -y
mamba create -n tbprofiler_env -c bioconda -c conda-forge python=3.9 tb-profiler -y
conda activate tbprofiler_env
tb-profiler --version
```

Snippy
```bash
conda deactivate
conda create -n snippy_env -c bioconda -c conda-forge snippy -y
conda activate snippy_env
snippy --version
```
Qualimap
```bash
conda create -n qualimap_env -c bioconda -c conda-forge qualimap=2.2.2d -y
conda activate qualimap_env
qualimap --help

```
tb_variant_filter (COMBAT-TB)
```bash
conda install mamba -c conda-forge
mamba create -n tb_variant_filter_env python=3.9
mamba activate tb_variant_filter_env
mamba install -c bioconda tb_variant_filter

```
BCFtools / SAMtools
```bash
conda create -n bcftools_env -c bioconda -c conda-forge bcftools=1.17 samtools=1.17 -y
conda activate bcftools_env
bcftools --version
samtools --version

```
MAFFT
```bash
conda create -n mafft_env -c bioconda -c conda-forge mafft=7.505 -y
conda activate mafft_env
mafft --version

```
IQ-TREE
```bash
conda create -n iqtree_env -c bioconda -c conda-forge iqtree=2.2.2.3 -y
conda activate iqtree_env
iqtree2 --version
```
iTOL
Web-based tool: https://itol.embl.de



# 1ï¸âƒ£ Explore Raw FASTQ Files

### FASTQ summary


Before starting any job we have changed to our working directory
```bash
cd /home/betselot/Project_data/TB/NTRL
```
We renamed all paired-end sequencing files so that forward reads end with `_1.fastq.gz` and reverse reads end with `_2.fastq.gz`, following the standard naming convention expected by most bioinformatics tools.

```bash
for f in raw_data/*_R1.fastq.gz; do
    base=${f%_R1.fastq.gz}
    mv "$f" "${base}_1.fastq.gz"
done

for f in raw_data/*_R2.fastq.gz; do
    base=${f%_R2.fastq.gz}
    mv "$f" "${base}_2.fastq.gz"
done
```
Paired-end FASTQ files are first inspected using simple command-line tools.

### 1. Peek at the first few reads
Before analyzing sequencing data, itâ€™s useful to inspect the first few reads in each FASTQ file. This allows a quick check of file format, sequence identifiers, and the general structure of paired-end reads:

```bash
zcat raw_data/ETRS-003_1.fastq.gz | head -n 16
zcat raw_data/ETRS-003_2.fastq.gz | head -n 16
```
<details>
<summary>ğŸ” Inspect First Reads of FASTQ Files</summary>
- `zcat` is a Linux/Unix command used to **view the contents of compressed files** without manually uncompressing them.  
- Works primarily with `.gz` files (gzip-compressed).  
- Unlike `gunzip`, it **prints the uncompressed data to standard output** instead of creating a new file.  
- Example workflow:  
  - `zcat raw_data/ETRS-003_1.fastq.gz | head -n 16` â†’  Show the first 16 lines of R1 (forward) FASTQ file of a compressed file without extracting it. 
- Each read in FASTQ format consists of 4 lines:  
  1. Header line (`@`) with read ID  
  2. Sequence line  
  3. Separator line (`+`)  
  4. Quality scores line  
  - Useful for:  
  - Checking file format  
  - Inspecting sequences and quality  
  - Ensuring R1 and R2 files are correctly paired  
</details>


### 2. Count total reads
To determine the total number of reads in each FASTQ file, divide the total number of lines by 4, since each read consists of four lines (identifier, sequence, optional identifier, and quality scores). This ensures paired-end files are consistent and provides an initial check on sequencing depth:
For single sample
```bash
echo $(( $(zcat raw_data/ETRS-003_1.fastq.gz | wc -l) / 4 ))
echo $(( $(zcat raw_data/ETRS-003_2.fastq.gz | wc -l) / 4 ))
```
<details>
<summary>ğŸ“Š Counting Reads in FASTQ Files</summary>

- `zcat raw_data/SRR28821350_1.fastq.gz` â†’ Decompresses the R1 FASTQ file and sends its content to standard output.  
- `| wc -l` â†’ Counts the total number of lines in the decompressed file.  
- `$(( ... / 4 ))` â†’ **Arithmetic expansion** in Bash: evaluates the expression inside `$(( ))`.  
  - Divides the total number of lines by 4 because **each sequencing read occupies 4 lines** in a FASTQ file:  
    1. Header line (`@`)  
    2. Sequence line  
    3. Separator line (`+`)  
    4. Quality scores line  
- `echo` â†’ Prints the resulting number of reads.  
- The second command works the same for the R2 (reverse) FASTQ file.  
- Overall, these commands give a **quick read count** for paired-end sequencing files.  

</details>


For batch processing 
##### Step 1: Open a new script
```bash
nano count_reads.sh
```
##### Step 2: Paste the following code
This script counts reads in paired-end FASTQ files and saves results to a CSV.

```bash
#!/bin/bash
set -euo pipefail

INDIR="raw_data"
OUTDIR="csv_output"
mkdir -p "$OUTDIR"

OUTFILE="$OUTDIR/fastq_read_counts.csv"
echo "Sample,R1_reads,R2_reads" > "$OUTFILE"
echo "ğŸ“Š Counting reads in FASTQ files from '$INDIR'..."

for R1 in "$INDIR"/*_1.fastq.gz; do
    [[ -f "$R1" ]] || continue
    SAMPLE=$(basename "$R1" | sed -E 's/_1\.fastq\.gz//')
    R2="$INDIR/${SAMPLE}_2.fastq.gz"

    R1_COUNT=$(( $(zcat "$R1" | wc -l) / 4 ))
    if [[ -f "$R2" ]]; then
        R2_COUNT=$(( $(zcat "$R2" | wc -l) / 4 ))
    else
        R2_COUNT="NA"
    fi

    echo "$SAMPLE,$R1_COUNT,$R2_COUNT" >> "$OUTFILE"
    echo "âœ… $SAMPLE â†’ R1: $R1_COUNT | R2: $R2_COUNT"
done

echo "ğŸ‰ All done! Read counts saved to '$OUTFILE'"

```

<details>
<summary>ğŸ“„ Script Overview</summary>

Counts reads in paired-end FASTQ files and outputs a CSV.  

- ğŸ›¡ `set -euo pipefail` â†’ safe script execution  
- ğŸ“‚ `INDIR="raw_data"` â†’ input FASTQ folder  
- ğŸ“‚ `OUTDIR="csv_output"` â†’ output CSV folder (auto-created)  
- ğŸ“ `OUTFILE="$OUTDIR/fastq_read_counts.csv"` â†’ CSV to store counts  
- ğŸ”„ Loop through R1 files: `for R1 in "$INDIR"/*_1.fastq.gz`  
- â›” Skip missing files: `[[ -f "$R1" ]] || continue`  
- ğŸ· Extract sample name: `basename "$R1" | sed -E 's/_1\.fastq\.gz//'`  
- ğŸ”— Find R2: `R2="$INDIR/${SAMPLE}_2.fastq.gz"`  
- ğŸ“Š Count reads: `R1_COUNT=$(zcat "$R1" | wc -l /4)`, `R2_COUNT=â€¦` or `"NA"`  
- â• Append to CSV: `echo "$SAMPLE,$R1_COUNT,$R2_COUNT" >> "$OUTFILE"`  
- âœ… Progress message per sample  
- ğŸ‰ Completion message  

</details>


### 3. Base composition
Checking the nucleotide composition of each FASTQ file helps assess sequencing quality. Balanced proportions of A, T, G, and C indicate high-quality data with minimal bias. This script counts the occurrence of each base in both paired-end files:

```bash
#!/bin/bash
for fq in raw_data/ETRS-003_1.fastq.gz raw_data/ETRS-003_2.fastq.gz; do
    [ -f "$fq" ] || continue
    echo "Counting bases in $fq..."
    zcat "$fq" | awk 'NR%4==2 { for(i=1;i<=length($0);i++) b[substr($0,i,1)]++ } 
    END { for(base in b) print base, b[base] }'
    echo "----------------------"
done
```
<details>
<summary>ğŸ§¬ Base Counting Script Overview</summary>

Counts nucleotide bases in specific FASTQ files.  

- ğŸ”„ Loops through specified FASTQ files: `raw_data/ETRS-003_1.fastq.gz` and `_2.fastq.gz`  
- â›” Skips missing files: `[ -f "$fq" ] || continue`  
- ğŸ–¨ Prints message: `"Counting bases in $fq..."`  
- ğŸ§® Counts bases using `awk`:  
  - `NR%4==2` â†’ selects sequence lines  
  - Loops over each character to tally `A, C, G, T, N`  
  - Prints counts per base at the end  
- â– Prints separator `"----------------------"` between files  

</details>



### 4. Quality score summary
FASTQ files encode base quality scores on the 4th line of every read. Checking these scores provides an initial assessment of sequencing quality before trimming or downstream analysis:
First 10 quality lines
```bash
zcat raw_data/ETRS-003_1.fastq.gz | sed -n '4~4p' | head -n 10
zcat raw_data/ETRS-003_2.fastq.gz | sed -n '4~4p' | head -n 10
```
<details>
<summary>ğŸ” FASTQ Quality Lines Preview</summary>

Displays the first 10 quality lines from paired-end FASTQ files.  

- ğŸ”„ Reads R1 and R2 FASTQ files using `zcat`  
- ğŸ“ `sed -n '4~4p'` â†’ selects every 4th line (quality scores)  
- ğŸ‘€ `head -n 10` â†’ shows only the first 10 quality lines  
- ğŸ–¨ Prints output directly to the terminal  

</details>


FASTQ quality scores are encoded as ASCII characters. Counting the occurrence of each character provides a quantitative overview of base quality across the reads:
```bash
zcat raw_data/ETRS-003_1.fastq.gz | sed -n '4~4p' | awk '{for(i=1;i<=length($0);i++){q[substr($0,i,1)]++}} END{for (k in q) print k,q[k]}'
zcat raw_data/ETRS-003_2.fastq.gz | sed -n '4~4p' | awk '{for(i=1;i<=length($0);i++){q[substr($0,i,1)]++}} END{for (k in q) print k,q[k]}'
```
<details>
<summary>ğŸ“Š Quality Score Base Counting</summary>

Counts the occurrence of each quality score character in FASTQ files.  

- ğŸ”„ Reads R1 and R2 FASTQ files using `zcat`  
- ğŸ“ `sed -n '4~4p'` â†’ selects every 4th line (quality score lines)  
- ğŸ§® `awk` â†’ loops over each character in quality lines to tally counts  
- ğŸ–¨ Prints counts per quality score character for each file  

</details>


### 5.   Checking FASTQ Pairing 

We ensured all our FASTQ files are correctly paired before running any bioinformatics analysis.

##### Step 1: Create the script
```bash
nano check_fastq_pairs.sh
```
##### Step 2: Paste the following into `check_fastq_pairs.sh`
```bash
#!/bin/bash
set -euo pipefail

INDIR="raw_data"
[[ "$(basename "$PWD")" != "raw_data" ]] && cd "$INDIR" || { echo "âŒ raw_data directory not found"; exit 1; }

echo "ğŸ” Checking FASTQ pairings in $PWD ..."

MISSING=false
PAIRED_COUNT=0
TOTAL_COUNT=0

for R1 in *_1.fastq.gz; do
    [[ -f "$R1" ]] || continue
    TOTAL_COUNT=$((TOTAL_COUNT+1))
    SAMPLE=${R1%_1.fastq.gz}

    if [[ -f "${SAMPLE}_2.fastq.gz" ]]; then
        echo "âœ… $SAMPLE â€” paired"
        PAIRED_COUNT=$((PAIRED_COUNT+1))
    else
        echo "âŒ $SAMPLE â€” missing _2.fastq.gz file"
        MISSING=true
    fi
done

echo -e "\nTotal samples checked: $TOTAL_COUNT"
echo "Correctly paired samples: $PAIRED_COUNT"
$MISSING && echo "âš  Some samples are missing pairs. Fix before running fastp." || echo "âœ… All FASTQ files are correctly paired."

```
<details>
<summary>ğŸ”— FASTQ Pairing Check Script</summary>

Verifies that all R1 FASTQ files have corresponding R2 files.  

- ğŸ›¡ `set -euo pipefail` â†’ safe script execution  
- ğŸ“‚ `INDIR="raw_data"` â†’ input FASTQ folder  
- ğŸ”„ Checks if current directory is `raw_data`; cd if not  
- ğŸ” Loops through all `_1.fastq.gz` files  
- ğŸ· Extracts sample name: `${R1%_1.fastq.gz}`  
- âœ… Prints `"paired"` if R2 exists; âŒ prints `"missing"` otherwise  
- ğŸ”¢ Tracks total and paired sample counts  
- âš  Summarizes results: missing pairs warning or all paired confirmation  

</details>


##### Step 4: Make the script executable
```bash
chmod +x check_fastq_pairs.sh
```
##### Step 5: Run the script
```bash
./check_fastq_pairs.sh
```
> **Tip:** Ensure all R1/R2 naming conventions in your directory match the patterns used in the script.  
> You can adjust the patterns (`*_1.fastq.gz`, `*_R1.fastq.gz`, etc.) if needed.

Calculating Minimum, Maximum, and Average Read Lengths for Paired-End Reads

### 6.   Checking raw FASTQ Read Length Summary

<details>
<summary>ğŸ“ Read Length Summary â€“ Importance and Benefits</summary>

Before performing any downstream bioinformatics analysis, it is important to understand the quality and characteristics of your sequencing data. One key metric is the **read length** of FASTQ files.  

## ğŸ”¹ Why read length matters  
- **Minimum read length:** Identifies very short reads that may result from sequencing errors or trimming. Extremely short reads can cause mapping errors or low-quality variant calls.  
- **Maximum read length:** Confirms whether reads were sequenced to the expected length and detects unusually long reads that may indicate adapter contamination or sequencing artifacts.  
- **Average read length:** Provides an overall measure of sequencing quality and consistency across the dataset.  

## ğŸ”¹ Importance in paired-end sequencing  
Calculating these metrics for **both R1 and R2 reads** is crucial:  
- Ensures both reads in a pair are of comparable lengths â†’ essential for accurate alignment and variant calling.  
- Detects discrepancies between forward and reverse reads that may indicate technical issues during sequencing or library preparation.  
- Allows early filtering of problematic samples before computationally intensive steps such as mapping, variant calling, or assembly.  

## ğŸ”¹ Benefits of summarizing into a CSV  
By compiling read lengths into a **CSV file**, you can:  
- Quickly inspect and compare samples.  
- Identify outliers or problematic datasets.  
- Make informed decisions on trimming, filtering, or quality control.  
- Improve reliability and reproducibility of downstream analyses.  

</details>
---

##### Step 1:  Open nano to create a new script
```bash
nano fastq_read_length_summary.sh
```
##### Step 2: Paste the following code into nano

```bash
#!/bin/bash
set -euo pipefail

FASTQ_DIR="raw_data"
OUTDIR="csv_output"
OUTPUT_CSV="${OUTDIR}/read_length_summary.csv"

mkdir -p "$OUTDIR"

echo "Sample,R1_min,R1_max,R1_avg,R2_min,R2_max,R2_avg" > "$OUTPUT_CSV"

for R1 in "$FASTQ_DIR"/*_1.fastq.gz; do
    [[ -f "$R1" ]] || continue
    SAMPLE=$(basename "$R1" | sed -E 's/_1\.fastq\.gz//')
    R2="$FASTQ_DIR/${SAMPLE}_2.fastq.gz"

    if [[ -f "$R2" ]]; then
        echo "Processing sample $SAMPLE"

        calc_stats() {
            zcat "$1" | awk 'NR%4==2 {len=length($0); sum+=len; if(min==""){min=len}; if(len<min){min=len}; if(len>max){max=len}; count++} END{avg=sum/count; printf "%d,%d,%.2f", min, max, avg}'
        }

        STATS_R1=$(calc_stats "$R1")
        STATS_R2=$(calc_stats "$R2")

        echo "$SAMPLE,$STATS_R1,$STATS_R2" >> "$OUTPUT_CSV"
    else
        echo "âš  Missing _2.fastq.gz for $SAMPLE, skipping."
    fi
done

echo "âœ… Read length summary saved to $OUTPUT_CSV"

```
<details>
<summary>ğŸ“ FASTQ Read Length Summary Script</summary>

Calculates min, max, and average read lengths for paired-end FASTQ files.  

- ğŸ›¡ `set -euo pipefail` â†’ safe script execution  
- ğŸ“‚ `FASTQ_DIR="raw_data"` â†’ input FASTQ folder  
- ğŸ“‚ `OUTDIR="csv_output"` â†’ output folder; auto-created  
- ğŸ“ `OUTPUT_CSV="read_length_summary.csv"` â†’ stores read length stats  
- ğŸ”„ Loops through all `_1.fastq.gz` files  
- ğŸ· Extracts sample name: `basename ... | sed -E 's/_1\.fastq\.gz//'`  
- ğŸ”— Finds corresponding R2 file; skips if missing  
- ğŸ§® `calc_stats()` â†’ calculates min, max, average read lengths using `awk`  
- â• Appends results to CSV: `Sample,R1_min,R1_max,R1_avg,R2_min,R2_max,R2_avg`  
- âœ… Prints completion message when finished  

</details>


##### Step 3: Save and exit nano
Press Ctrl + O â†’ Enter (to write the file)
Press Ctrl + X â†’ Exit nano
##### Step 4: Make the script executable
```bash
chmod +x fastq_read_length_summary.sh
```
##### Step 5: Run the script
```bash
./fastq_read_length_summary.sh
```

# 2ï¸âƒ£ Quality Control & Trimming with **FASTP**

<details>
<summary>âš¡ FASTQ Preprocessing with fastp</summary>

[`fastp`](https://github.com/OpenGene/fastp) is a widely used **FASTQ preprocessor** for quality control (QC), read trimming, and adapter removal. It is efficient, multithreaded, and provides both **JSON and HTML reports** for each sample.  

In this pipeline, `fastp` ensures that only **high-quality reads** are retained before mapping and variant calling. High-quality read preprocessing is crucial in *Mycobacterium tuberculosis* (TB) WGS analysis because poorly trimmed or unfiltered reads can lead to:
- False-positive SNP calls  
- Mapping errors, especially in repetitive regions (e.g., PE/PPE genes)  
- Biased coverage, affecting downstream variant interpretation  

### Advantages of fastp over Trimmomatic and other tools
- **All-in-one solution**: Handles trimming, adapter detection, filtering, and QC in a single step.  
- **Automatic adapter detection**: Reduces human error, especially for large TB projects with mixed sequencing batches.  
- **Speed and multithreading**: Written in C++, much faster than Java-based Trimmomatic.  
- **Comprehensive QC output**: HTML (interactive) and JSON (machine-readable) reports for quality distribution, duplication rates, adapter content, and polyG/polyX tails.  
- **Better polyG/polyX handling**: Important for Illumina NovaSeq/NextSeq data.  
- **UMI support**: Useful for advanced TB sequencing protocols using Unique Molecular Identifiers.  
- **Minimal parameter tuning**: Default settings are optimized, reducing manual adjustments.  

### Why this matters for TB analysis
- **Accurate SNP calling**: TB drug-resistance prediction relies on high-confidence SNPs.  
- **Low genetic diversity detection**: TB isolates often differ by only a few SNPs. Filtering errors while retaining true variants is critical.  
- **Scalability for large cohorts**: Efficient, reproducible preprocessing is essential for thousands of public TB isolates from ENA/NCBI.  

</details>

---

### Steps to Run FASTP
##### Step 1: **Open nano to create the script**
```bash
nano run_fastp.sh
```
##### Step 2: Paste the following code into nano
```bash
#!/bin/bash
set -euo pipefail

INDIR="raw_data"
OUTDIR="fastp_results_min_50"
mkdir -p "$OUTDIR"

SAMPLES=()

for R1 in "$INDIR"/*_1.fastq.gz; do
    [[ -f "$R1" ]] || continue

    SAMPLE=$(basename "$R1" | sed -E 's/_1\.fastq\.gz//')
    R2="$INDIR/${SAMPLE}_2.fastq.gz"

    if [[ ! -f "$R2" ]]; then
        echo "âš  No _2.fastq.gz file found for $SAMPLE â€” skipping."
        continue
    fi

    if [[ -f "$OUTDIR/${SAMPLE}_1.trim.fastq.gz" && -f "$OUTDIR/${SAMPLE}_2.trim.fastq.gz" ]]; then
        echo "â© Skipping $SAMPLE (already processed)."
        continue
    fi

    SAMPLES+=("$SAMPLE,$R1,$R2")
done

if [[ ${#SAMPLES[@]} -eq 0 ]]; then
    echo "âŒ No paired FASTQ files found in $INDIR"
    exit 1
fi

THREADS=$(nproc)
FASTP_THREADS=$(( THREADS / 2 ))

run_fastp() {
    SAMPLE=$1
    R1=$2
    R2=$3
    echo "âœ… Processing sample: $SAMPLE"
    fastp \
        -i "$R1" \
        -I "$R2" \
        -o "$OUTDIR/${SAMPLE}_1.trim.fastq.gz" \
        -O "$OUTDIR/${SAMPLE}_2.trim.fastq.gz" \
        -h "$OUTDIR/${SAMPLE}_fastp.html" \
        -j "$OUTDIR/${SAMPLE}_fastp.json" \
        --length_required 50 \
        --qualified_quality_phred 20 \
        --detect_adapter_for_pe \
        --thread $FASTP_THREADS \
        &> "$OUTDIR/${SAMPLE}_fastp.log"
}

export -f run_fastp
export OUTDIR FASTP_THREADS

printf "%s\n" "${SAMPLES[@]}" | parallel -j 3 --colsep ',' run_fastp {1} {2} {3}

echo "ğŸ‰ Completed fastp for $(ls "$OUTDIR"/*_fastp.json | wc -l) samples."
```
<details>
<summary>âœ‚ï¸ FASTQ Trimming with fastp Script</summary>

Trims paired-end FASTQ files using `fastp` and generates JSON/HTML reports.  

- ğŸ›¡ `set -euo pipefail` â†’ safe script execution  
- ğŸ“‚ `INDIR="raw_data"` â†’ input FASTQ folder  
- ğŸ“‚ `OUTDIR="fastp_results_min_50"` â†’ trimmed output folder; auto-created  
- ğŸ”„ Loops through `_1.fastq.gz` files  
- ğŸ· Extracts sample name and finds corresponding `_2.fastq.gz`; skips missing or already processed samples  
- ğŸ§µ Determines threads: `FASTP_THREADS=$((nproc / 2))`  
- âœ… `run_fastp()` â†’ runs `fastp` with:  
  - min read length 50 (`--length_required 50`)  
  - quality Phred â‰¥20 (`--qualified_quality_phred 20`)  
  - paired-end adapter detection (`--detect_adapter_for_pe`)  
  - outputs: trimmed FASTQ, JSON, HTML, and log files  
- âš¡ Uses GNU `parallel` to process multiple samples concurrently  
- ğŸ‰ Prints completion message with total processed samples  

</details>

##### Step 3: Save & exit nano
Press CTRL+O, Enter (save)
Press CTRL+X (exit)
##### Step 4: Make the script executable
```bash
chmod +x run_fastp.sh
```
##### Step 5: Activate your conda env and run
```bash
conda activate fastp_env
./run_fastp.sh
```

Count R1 trimmed files
```bash
ls -lth fastp_results_min_50/*_1.trim.fastq.gz | wc -l
```
Count R2 trimmed files
```bash
ls -lth fastp_results_min_50/*_2.trim.fastq.gz | wc -l
```

View first 10 quality lines in trimmed FASTQ

```bash
 Show first 10 quality lines from R1
```bash
echo "ğŸ”¹ First 10 quality lines from ETRS-003_1 (R1):"
zcat fastp_results_min_50/ETRS-003_1.trim.fastq.gz \
| sed -n '4~4p' \
| head -n 10 \
| awk '{print "âœ… " $0}'
```
 Show first 10 quality lines from R2
```bash
echo "ğŸ”¹ First 10 quality lines from ETRS-003_2 (R2):"
zcat fastp_results_min_50/ETRS-003_2.trim.fastq.gz \
| sed -n '4~4p' \
| head -n 10 \
| awk '{print "âœ… " $0}'

```
<details>
<summary>ğŸ” Preview Trimmed FASTQ Quality Lines</summary>

Displays the first 10 quality score lines from trimmed FASTQ files.  

- ğŸ”¹ Reads trimmed R1 and R2 FASTQ files (`fastp_results_min_50/*.trim.fastq.gz`)  
- ğŸ“ `sed -n '4~4p'` â†’ selects every 4th line (quality scores)  
- ğŸ‘€ `head -n 10` â†’ shows only the first 10 lines  
- âœ… `awk '{print "âœ… " $0}'` â†’ adds checkmark for visualization  
- ğŸ–¨ Prints output to terminal with clear labels for R1 and R2  

</details>


Count ASCII characters in quality lines:
```bash
    Count base composition in R1
```bash
zcat fastp_results_min_50/ETRS-003_1.trim.fastq.gz \
| sed -n '4~4p' \
| awk '{
    for(i=1;i<=length($0);i++){ q[substr($0,i,1)]++ }
} END {
    for (k in q) print k, q[k]
}' \
| awk '{print "âœ… Base " $1 ": " $2 " occurrences"}'
```
   Count base composition in R2
```bash
zcat fastp_results_min_50/ETRS-003_2.trim.fastq.gz \
| sed -n '4~4p' \
| awk '{
    for(i=1;i<=length($0);i++){ q[substr($0,i,1)]++ }
} END {
    for (k in q) print k, q[k]
}' \
| awk '{print "âœ… Base " $1 ": " $2 " occurrences"}'
```
<details>
<summary>ğŸ“Š Quality Score Base Counts (Trimmed FASTQ)</summary>

Counts occurrences of each quality score character in trimmed FASTQ files.  

- ğŸ”¹ Reads trimmed R1 and R2 FASTQ files (`fastp_results_min_50/*.trim.fastq.gz`)  
- ğŸ“ `sed -n '4~4p'` â†’ selects every 4th line (quality score lines)  
- ğŸ§® `awk` â†’ loops over each character to tally occurrences  
- âœ… `awk '{print "âœ… Base " $1 ": " $2 " occurrences"}'` â†’ formats counts for clear visualization  
- ğŸ–¨ Prints counts per quality score for R1 and R2  

</details>


For batch processing trimmed FASTQ
##### Step 1: Open a new script
```bash
nano count_trimmed_reads.sh
```
##### Step 2: Paste the following code
This script counts reads in trimmed paired-end FASTQ files and saves results to a CSV.
```bash
#!/bin/bash
set -euo pipefail

INDIR="fastp_results_min_50"
OUTDIR="csv_output"
mkdir -p "$OUTDIR"

OUTFILE="$OUTDIR/trimmed_read_counts.csv"
echo "Sample,R1_reads,R2_reads" > "$OUTFILE"
echo "ğŸ“Š Counting reads in trimmed FASTQ files from '$INDIR'..."

for R1 in "$INDIR"/*_1.trim.fastq.gz; do
    [[ -f "$R1" ]] || continue
    SAMPLE=$(basename "$R1" | sed -E 's/_1\.trim\.fastq\.gz//')
    R2="$INDIR/${SAMPLE}_2.trim.fastq.gz"

    if [[ -f "$R2" ]]; then
        R1_COUNT=$(( $(zcat "$R1" | wc -l) / 4 ))
        R2_COUNT=$(( $(zcat "$R2" | wc -l) / 4 ))
    else
        R1_COUNT=$(( $(zcat "$R1" | wc -l) / 4 ))
        R2_COUNT="NA"
        echo "âš  Missing _2 file for $SAMPLE"
    fi

    echo "$SAMPLE,$R1_COUNT,$R2_COUNT" >> "$OUTFILE"
    echo "âœ… $SAMPLE â†’ R1: $R1_COUNT | R2: $R2_COUNT"
done

echo "ğŸ‰ All done! Read counts saved to '$OUTFILE'"

```
<details>
<summary>ğŸ“Š Trimmed FASTQ Read Counts</summary>

Counts reads in paired trimmed FASTQ files and outputs a CSV.  

- ğŸ›¡ `set -euo pipefail` â†’ safe script execution  
- ğŸ“‚ `INDIR="fastp_results_min_50"` â†’ folder with trimmed FASTQ files  
- ğŸ“‚ `OUTDIR="csv_output"` â†’ CSV output folder; auto-created  
- ğŸ“ `OUTFILE="trimmed_read_counts.csv"` â†’ stores read counts per sample  
- ğŸ”„ Loops through `_1.trim.fastq.gz` files  
- ğŸ· Extracts sample name: `basename ... | sed -E 's/_1\.trim\.fastq\.gz//'`  
- ğŸ”— Finds corresponding R2 file; if missing, R2 count = "NA"  
- ğŸ§® Counts reads: `$(zcat file | wc -l) / 4`  
- â• Appends counts to CSV: `Sample,R1_reads,R2_reads`  
- âœ… Prints progress for each sample  
- ğŸ‰ Prints completion message when done  

</details>

##### Step 3: Save and exit nano
Press Ctrl + O â†’ Enter (to write the file)
Press Ctrl + X â†’ Exit nano

##### Step 4: Make the script executable
```bash
chmod +x count_trimmed_reads.sh
```
##### Step 5: Run the script
```bash
./count_trimmed_reads.sh
```
Using zcat and wc (line counts)
Each read in FASTQ has 4 lines, so dividing by 4 gives read count. 
```bash
RAW_R1_COUNT=$(( $(zcat raw_data/ET3_S55_1.fastq.gz | wc -l) / 4 ))
RAW_R2_COUNT=$(( $(zcat raw_data/ET3_S55_2.fastq.gz | wc -l) / 4 ))

TRIM_R1_COUNT=$(( $(zcat fastp_results_min_50/ET3_S55_1.trim.fastq.gz | wc -l) / 4 ))
TRIM_R2_COUNT=$(( $(zcat fastp_results_min_50/ET3_S55_2.trim.fastq.gz | wc -l) / 4 ))

echo "R1 trimmed reads: $(( RAW_R1_COUNT - TRIM_R1_COUNT ))"
echo "R2 trimmed reads: $(( RAW_R2_COUNT - TRIM_R2_COUNT ))"
```
<details>
<summary>âœ‚ï¸ Read Trimming Summary</summary>

Calculates the number of reads removed during trimming for a sample.  

- ğŸ”¹ Counts reads in raw FASTQ files: `_1.fastq.gz` and `_2.fastq.gz`  
- ğŸ”¹ Counts reads in trimmed FASTQ files: `_1.trim.fastq.gz` and `_2.trim.fastq.gz`  
- ğŸ§® Calculates trimmed reads: `RAW_COUNT - TRIM_COUNT` for R1 and R2  
- ğŸ–¨ Prints the number of reads removed during trimming  

</details>


read length summary on trimmed FASTQ files
##### Step 1: Open nano to create a new script
```bash
nano trimmed_fastq_read_length_summary.sh
```
##### Step 2: Paste the following code into nano
```bash
#!/bin/bash
set -euo pipefail

FASTQ_DIR="fastp_results_min_50"
OUTDIR="csv_output"
OUTPUT_CSV="${OUTDIR}/trimmed_read_length_summary.csv"

mkdir -p "$OUTDIR"

echo "Sample,R1_min,R1_max,R1_avg,R2_min,R2_max,R2_avg" > "$OUTPUT_CSV"

for R1 in "$FASTQ_DIR"/*_1.trim.fastq.gz; do
    [[ -f "$R1" ]] || continue
    SAMPLE=$(basename "$R1" _1.trim.fastq.gz)
    R2="${FASTQ_DIR}/${SAMPLE}_2.trim.fastq.gz"

    if [[ -f "$R2" ]]; then
        echo "Processing sample $SAMPLE"

        calc_stats() {
            zcat "$1" | awk 'NR%4==2 {
                len=length($0)
                sum+=len
                if(min==""){min=len}
                if(len<min){min=len}
                if(len>max){max=len}
                count++
            } END {
                avg=sum/count
                printf "%d,%d,%.2f", min, max, avg
            }'
        }

        STATS_R1=$(calc_stats "$R1")
        STATS_R2=$(calc_stats "$R2")

        echo "$SAMPLE,$STATS_R1,$STATS_R2" >> "$OUTPUT_CSV"
    else
        echo "âš  Missing _2 file for $SAMPLE, skipping."
    fi
done

echo "âœ… Trimmed read length summary saved to $OUTPUT

```
<details>
<summary>ğŸ“ Trimmed FASTQ Read Length Summary</summary>

Calculates min, max, and average read lengths for paired trimmed FASTQ files.  

- ğŸ›¡ `set -euo pipefail` â†’ safe script execution  
- ğŸ“‚ `FASTQ_DIR="fastp_results_min_50"` â†’ folder with trimmed FASTQ files  
- ğŸ“‚ `OUTDIR="csv_output"` â†’ output folder; auto-created  
- ğŸ“ `OUTPUT_CSV="trimmed_read_length_summary.csv"` â†’ stores read length stats  
- ğŸ”„ Loops through `_1.trim.fastq.gz` files  
- ğŸ· Extracts sample name: `basename "$R1" _1.trim.fastq.gz`  
- ğŸ”— Finds corresponding R2 file; skips sample if missing  
- ğŸ§® `calc_stats()` â†’ calculates min, max, average read lengths using `awk`  
- â• Appends results to CSV: `Sample,R1_min,R1_max,R1_avg,R2_min,R2_max,R2_avg`  
- âœ… Prints completion message when done  

</details>

##### Step 3: Save and exit nano
Press Ctrl + O â†’ Enter
Press Ctrl + X â†’ Exit
##### Step 4: Make the script executable
```
chmod +x trimmed_fastq_read_length_summary.sh
```
##### Step 5: Run the script
```bash
./trimmed_fastq_read_length_summary.sh
```
# 3ï¸âƒ£ Aggregate QC Reports with **MultiQC**
[`MultiQC`](https://github.com/MultiQC/MultiQC) 

<details>
<summary>ğŸ“Š Aggregating QC with MultiQC</summary>

After preprocessing with `fastp`, we often generate dozens or hundreds of per-sample QC reports (`.html` and `.json`). Instead of checking each report manually, **MultiQC** aggregates all results into a single interactive HTML report.  

### Why we use MultiQC in TB analysis
- **Aggregated QC overview**: Summarizes all `fastp` results in one place.  
- **Consistency check**: Quickly detects outlier samples (e.g., unusually short reads, poor quality, or failed trimming).  
- **Scalable**: Handles hundreds or thousands of TB isolates efficiently.  
- **Standardized reporting**: Facilitates sharing results across teams or for publications.  

</details>

---

### Script: Run MultiQC
##### Step 1: **Open nano to create the script `run_multiqc.sh`
```bash
nano run_multiqc.sh
```
##### Step 2: Paste the following code into nano
```bash
#!/bin/bash
set -euo pipefail

INPUT_DIR="fastp_results_min_50"
OUTPUT_DIR="multiqc/fastp_multiqc"

mkdir -p "$OUTPUT_DIR"

if [ ! -d "$INPUT_DIR" ]; then
    echo "Error: Input directory '$INPUT_DIR' does not exist!"
    exit 1
fi

multiqc "$INPUT_DIR" -o "$OUTPUT_DIR"

echo "MultiQC report generated in '$OUTPUT_DIR'."
```
<details>
<summary>ğŸ“‘ Generate MultiQC Report</summary>

Generates a MultiQC report from trimmed FASTQ files.  

- ğŸ›¡ `set -euo pipefail` â†’ safe script execution  
- ğŸ“‚ `INPUT_DIR="fastp_results_min_50"` â†’ folder with trimmed FASTQ files  
- ğŸ“‚ `OUTPUT_DIR="multiqc/fastp_multiqc"` â†’ folder to store MultiQC report; auto-created  
- âš  Checks if input directory exists; exits if not  
- ğŸ”„ Runs `multiqc` on the input folder and outputs to the specified directory  
- ğŸ–¨ Prints confirmation message with report location  

</details>


##### Step 3: Save & exit nano
Press CTRL+O, Enter (save)
Press CTRL+X (exit)
##### Step 4: Make the script executable
```bash
chmod +x run_multiqc.sh
```
##### Step 5: Activate your conda env and run
```bash
conda activate multiqc_env
./run_multiqc.sh
```
# 4ï¸âƒ£ Drug Resistance Screening with **TB-Profiler**
Before running [`TB-Profiler`](https://github.com/jodyphelan/TBProfiler), we perform quality checks on raw FASTQ files and exclude samples with extremely low-quality reads. However, since TB-Profiler internally uses Trimmomatic to trim adapters and low-quality bases, it is not necessary to pre-trim the reads. Therefore, only quality-checked FASTQ files are provided as input to TB-Profiler, allowing it to handle trimming and variant calling internally.

<details>
<summary>ğŸ§¬ TB-Profiler: Variant Calling, Lineage, and Drug Resistance</summary>

**TB-Profiler** is a specialized tool for *Mycobacterium tuberculosis* whole-genome sequencing (WGS) data. It performs **variant calling, lineage determination, and drug resistance prediction** in a single pipeline.

### Why we use TB-Profiler
- ğŸ§ª **Drug Resistance Prediction** â†’ Detects known resistance mutations for first- and second-line TB drugs.  
- ğŸŒ **Lineage Typing** â†’ Classifies isolates into recognized TB lineages (e.g., Lineage 1â€“7).  
- ğŸ”„ **Flexible Input** â†’ Accepts FASTQ, BAM, or VCF files.  
- ğŸ“Š **Clear Outputs** â†’ Produces human-readable (`.txt`) and machine-readable (`.json`) reports.  
- âš¡ **Speed & Integration** â†’ Efficient and easily incorporated into TB genomics pipelines.  

### Why it matters
- Provides **actionable insights** for public health, including drug resistance and outbreak tracking.  
- Avoids manual cross-referencing with multiple TB resistance databases.  
- Ensures **standardized results** comparable across studies.  

</details>
 
---

## Steps
##### Step 1: Create or edit the script
```bash
nano run_tbprofiler.sh
```
##### Step 2: Paste the following code
```bash
#!/bin/bash
set -euo pipefail

FASTQ_DIR="raw_data"

echo "ğŸ“Š Starting TBProfiler runs for all samples in $FASTQ_DIR ..."

for R1 in "$FASTQ_DIR"/*_1.fastq.gz; do
    SAMPLE=$(basename "$R1" _1.fastq.gz)
    R2="$FASTQ_DIR/${SAMPLE}_2.fastq.gz"

    if [[ ! -f "$R2" ]]; then
        echo "âŒ Warning: missing paired file for $SAMPLE, skipping."
        continue
    fi

    echo "â–¶ï¸ Processing sample: $SAMPLE"

    tb-profiler profile \
        -1 "$R1" \
        -2 "$R2" \
        --threads 8 \
        --prefix "$SAMPLE" \
        --txt \
        --spoligotype

    echo "âœ… Finished $SAMPLE"
done

echo "ğŸ“Œ All samples processed!"
```
<details>
<summary>ğŸ¦  TBProfiler Sample Profiling</summary>

Runs TBProfiler on paired-end FASTQ files to profile Mycobacterium tuberculosis samples.  

- ğŸ›¡ `set -euo pipefail` â†’ safe script execution  
- ğŸ“‚ `FASTQ_DIR="raw_data"` â†’ folder with input FASTQ files  
- ğŸ”„ Loops through `_1.fastq.gz` files and finds corresponding `_2.fastq.gz`  
- âŒ Skips samples if paired R2 file is missing  
- â–¶ï¸ Runs `tb-profiler profile` with:  
  - 8 threads (`--threads 8`)  
  - output prefix as sample name (`--prefix`)  
  - TXT report (`--txt`)  
  - spoligotype analysis (`--spoligotype`)  
- âœ… Prints progress for each sample  
- ğŸ“Œ Prints completion message when all samples are processed  

</details>


##### Step 3: Save and exit nano
Press Ctrl + O â†’ Enter (to write the file)
Press Ctrl + X â†’ Exit nano

##### Step 4: Make the script executable
```bash
chmod +x run_tbprofiler.sh
```
##### Step 5: Activate environment and run
```bash
conda activate tbprofiler_env
./run_tbprofiler.sh
```
##### Step 6: move the output from tbprofiler into new directory 
make directory tbprofiler_results
```bash
mkdir -p tbprofiler_results
```
then move the tbprofiler output `bam`, `vcf` and `results` directories from current directory into tbprofiler_results directory 
```bash
mv bam vcf results tbprofiler_results/
echo "ğŸ“Œ All TBProfiler outputs moved to tbprofiler_results/"
```
##### Step 6: change directory to tbprofiler_results
```bash
cd ./tbprofiler_results
```
##### Step 7: Collate results
The results from numerous runs can be collated into one table using the following command:
```bash
tb-profiler collate
```
##### Step 8: Generate iTOL config for phylogeny visualization
If we want to visualize your phylogeny alongside drug-resistance types, lineages, and sublineages, run:
```bash
tb-profiler collate --itol
```
##### Step 8: Combine all tbprofiler.txt into one CSV
```bash
find . -name "tbprofiler.txt" \
| exec awk 'FNR==1 && NR!=1{next} {print}' {} + \
| sed 's/\t/,/g' \
> tbprofiler_collated.csv

echo "âœ… Collated all tbprofiler.txt files into tbprofiler_collated.csv"

```
<details>
<summary>ğŸ“‘ Collate TBProfiler Results</summary>

Merges all `tbprofiler.txt` outputs into a single CSV file.  

- ğŸ” Finds all `tbprofiler.txt` files recursively in the current directory  
- ğŸ§® Uses `awk` to skip repeated headers and combine contents  
- ğŸ”„ Replaces tabs with commas (`sed 's/\t/,/g'`)  
- â• Writes output to `tbprofiler_collated.csv`  
- âœ… Prints confirmation message after collation  

</details>


# 5ï¸âƒ£ Variant Calling with **Snippy**

[`Snippy`](https://github.com/tseemann/snippy) 
<details>
<summary>ğŸ§¬ Detailed Overview: Variant Calling with Snippy</summary>

`Snippy` is a rapid and reproducible pipeline designed for bacterial genome variant calling. It is particularly well-suited for **Mycobacterium tuberculosis (TB) WGS analysis** due to its efficiency and accuracy.

### Key Features
- **Reference-based mapping**: Maps raw sequencing reads directly to a reference genome (commonly *M. tuberculosis* H37Rv), ensuring accurate alignment even in repetitive regions.  
- **High-confidence SNP calling**: Detects single nucleotide polymorphisms (SNPs) with stringent filtering criteria to minimize false positives.  
- **Consensus sequence generation**: Produces high-quality consensus FASTA files per sample, which can be used for phylogenetic analysis or comparative genomics.  
- **Reproducibility**: Standardized workflow ensures consistent results across multiple samples and datasets.  
- **Lightweight and scalable**: Efficient for processing large cohorts of TB isolates without requiring extensive computational resources.  
- **Standardized output files**: Outputs include VCF files for SNPs, consensus FASTA sequences, and optional alignment summaries, facilitating downstream analyses such as:
  - Phylogenetic tree reconstruction  
  - Drug resistance prediction using TBProfiler  
  - Comparative genomics across multiple TB strains  
- **Easy integration**: Works seamlessly with other bioinformatics tools and pipelines, allowing automated WGS analysis workflows.  

### Importance in TB Analysis
- Ensures accurate variant detection for **drug resistance prediction**.  
- Allows monitoring of **microevolution** within TB outbreaks.  
- Provides **reliable consensus sequences** for large-scale phylogenetic studies.  

</details>

---

##### Step 1: Create the script
```bash
nano run_snippy.sh
```
##### Step 2: Paste the following into `run_snippy.sh`
```bash
#!/bin/bash
set -euo pipefail

REF="H37Rv.fasta"
FASTP_DIR="fastp_results_min_50"
OUTDIR="snippy_results"
THREADS=8
BWA_THREADS=30
JOBS=4

mkdir -p "$OUTDIR"

run_snippy_sample() {
    SAMPLE="$1"
    R1="${FASTP_DIR}/${SAMPLE}_1.trim.fastq.gz"
    R2="${FASTP_DIR}/${SAMPLE}_2.trim.fastq.gz"

    if [[ ! -f "$R1" || ! -f "$R2" ]]; then
        echo "âš  Missing R1/R2 for $SAMPLE"
        return
    fi

    echo "Running Snippy on sample: $SAMPLE"
    TMP_DIR="${OUTDIR}/${SAMPLE}_tmp"
    mkdir -p "$TMP_DIR"

    snippy --cpus "$THREADS" --outdir "$TMP_DIR" --ref "$REF" \
           --R1 "$R1" --R2 "$R2" --force --bwaopt "-T $BWA_THREADS"

    if [[ -f "$TMP_DIR/snps.vcf" ]]; then
        mv "$TMP_DIR/snps.vcf" "${OUTDIR}/${SAMPLE}.vcf"
    fi

    for f in "$TMP_DIR"/*; do
        base=$(basename "$f")
        case "$base" in
            *.consensus.fa) mv "$f" "${OUTDIR}/${SAMPLE}.consensus.fa" ;;
            *.bam) mv "$f" "${OUTDIR}/${SAMPLE}.bam" ;;
            *.bam.bai) mv "$f" "${OUTDIR}/${SAMPLE}.bam.bai" ;;
            *.tab) mv "$f" "${OUTDIR}/${SAMPLE}.snps.tab" ;;
        esac
    done

    rm -rf "$TMP_DIR"

    [[ -f "${OUTDIR}/${SAMPLE}.vcf" ]] && echo "âœ… Full VCF generated for $SAMPLE" || echo "âš  No VCF produced for $SAMPLE"
}

export -f run_snippy_sample
export REF FASTP_DIR OUTDIR THREADS BWA_THREADS

ls "${FASTP_DIR}"/*_1.trim.fastq.gz \
    | sed 's|.*/||; s/_1\.trim\.fastq\.gz//' \
    | parallel -j "$JOBS" run_snippy_sample {}

ls "${FASTP_DIR}"/*_1.trim.fastq.gz \
    | sed 's|.*/||; s/_1\.trim\.fastq\.gz//' | sort > fastq_samples.txt
ls "${OUTDIR}"/*.vcf 2>/dev/null \
    | sed 's|.*/||; s/\.vcf//' | sort > snippy_samples.txt

echo "FASTQ pairs count: $(wc -l < fastq_samples.txt)"
echo "Snippy outputs count: $(wc -l < snippy_samples.txt)"

if diff fastq_samples.txt snippy_samples.txt >/dev/null; then
    echo "âœ… All FASTQ pairs have corresponding Snippy results."
else
    echo "âš  Missing samples detected:"
    diff fastq_samples.txt snippy_samples.txt || true
fi

rm -f fastq_samples.txt snippy_samples.txt

echo "ğŸ¯ All steps completed!"
echo "Snippy results are in: ${OUTDIR}/"

```
<details>
<summary>ğŸ§¬ Snippy Variant Calling Pipeline</summary>

Performs variant calling on paired trimmed FASTQ files against a reference genome using Snippy.  

- ğŸ›¡ `set -euo pipefail` â†’ safe script execution  
- ğŸ“‚ Input/output directories:  
  - `FASTP_DIR="fastp_results_min_50"` â†’ trimmed FASTQ files  
  - `OUTDIR="snippy_results"` â†’ stores Snippy outputs  
- ğŸ§© Reference genome: `REF="H37Rv.fasta"`  
- ğŸ§µ Threads: `THREADS=8`, `BWA_THREADS=30`; parallel jobs: `JOBS=4`  
- ğŸ”„ `run_snippy_sample()` â†’ for each sample:  
  - Checks paired R1/R2 files  
  - Runs Snippy with specified threads and BWA options  
  - Moves outputs (`.vcf`, `.consensus.fa`, `.bam`, `.bam.bai`, `.snps.tab`) to output folder  
  - Cleans temporary directories  
  - Confirms if full VCF generated  
- âš¡ Uses GNU `parallel` to process multiple samples concurrently  
- ğŸ” Compares FASTQ sample list with Snippy outputs and reports missing samples  
- ğŸ¯ Prints completion message and output directory  

</details>


##### Step 3: Save and exit nano
Press Ctrl + O, then Enter (save)
Press Ctrl + X (exit)

##### Step 4: Make the script executable
```bash
chmod +x run_snippy.sh
```
##### Step 5: Activate environment and run
```bash
conda activate snippy_env
./run_snippy.sh
```
Check Snippy VCFs Before Variant Filtering
- `tb_variant_filter` relies on a correctly formatted VCF with the `#CHROM` line to parse variants.  
- VCFs missing this line will **fail** during filtering, causing errors like:  
  `Missing line starting with "#CHROM"`.  
- Running this check before variant filtering saves time and ensures downstream analysis runs smoothly.

This script ensures that all VCF files generated by **Snippy** contain the required `#CHROM` header line.
```bash
#!/bin/bash
OUTDIR="snippy_results"

echo "Checking Snippy VCFs in $OUTDIR ..."

for vcf in "$OUTDIR"/*.vcf; do
    SAMPLE=$(basename "$vcf")
    if grep -q "^#CHROM" "$vcf"; then
        echo "âœ… $SAMPLE contains #CHROM line"
    else
        echo "âš  $SAMPLE is missing #CHROM line"
    fi
done
```
<details>
<summary>ğŸ“ Snippy VCF Integrity Check</summary>

Verifies that Snippy-generated VCF files contain the mandatory `#CHROM` header line.  

- ğŸ“‚ `OUTDIR="snippy_results"` â†’ folder with Snippy VCF files  
- ğŸ”„ Loops through all `.vcf` files in the folder  
- âœ… Prints confirmation if `#CHROM` header line exists  
- âš  Warns if VCF is missing the header line  

</details>

# 6ï¸âƒ£ BAM Quality Check with **Qualimap**
[`Qualimap`](http://qualimap.conesalab.org/) 
<details>
<summary>ğŸ“ˆ BAM Quality Assessment with Qualimap</summary>

After generating BAM files with Snippy, it is crucial to evaluate their quality before downstream analyses. **Qualimap** is widely used for this purpose.

### Why we use Qualimap in TB genomics
- **Mapping quality evaluation**: Assesses alignment accuracy and read placement.  
- **Coverage distribution and GC content**: Detects uneven coverage or biases across the genome.  
- **Visual reports**: Generates **HTML** and **PDF** reports with clear QC metrics.  
- **Identify problematic samples**: Flags samples with low coverage, poor alignment, or uneven depth, which may affect variant calling.  
- **Integration with MultiQC**: QC results can be aggregated for large TB cohorts for batch reporting.  

</details>

---

##### Step 1: Create the script
```bash
nano run_qualimap.sh
```
#####  Step 2: Paste the following into `run_qualimap.sh`
```bash
#!/bin/bash
set -euo pipefail

SNIPPY_DIR="snippy_results"
QUALIMAP_OUT="qualimap_reports"
mkdir -p "$QUALIMAP_OUT"

for bam in "$SNIPPY_DIR"/*.bam; do
    sample=$(basename "$bam" .bam)
    echo "Running Qualimap BAM QC for sample: $sample"

    outdir="${QUALIMAP_OUT}/${sample}"
    mkdir -p "$outdir"

    qualimap bamqc \
        -bam "$bam" \
        -outdir "$outdir" \
        -outformat pdf:html \
        --java-mem-size=4G
done
```
<details>
<summary>ğŸ“Š Qualimap BAM QC</summary>

Performs quality control on BAM files using Qualimap and generates PDF/HTML reports.  

- ğŸ›¡ `set -euo pipefail` â†’ safe script execution  
- ğŸ“‚ `SNIPPY_DIR="snippy_results"` â†’ folder with BAM files  
- ğŸ“‚ `QUALIMAP_OUT="qualimap_reports"` â†’ output folder for QC reports; auto-created  
- ğŸ”„ Loops through all `.bam` files  
- ğŸ· Extracts sample name from BAM file  
- ğŸ§© Creates per-sample output directory  
- âš¡ Runs `qualimap bamqc` with:  
  - PDF and HTML output (`-outformat pdf:html`)  
  - Java memory allocation (`--java-mem-size=4G`)  
- ğŸ–¨ Prints progress for each sample  

</details>


##### Step 3: Save and exit nano

Press Ctrl + O, then Enter (save)
Press Ctrl + X (exit)
##### Step 4: Make the script executable
```bash
chmod +x run_qualimap.sh
```
##### Step 5: Activate environment and install GNU Parallel into your `qualimap_env`:
```bash
conda activate qualimap_env
```
##### Step 6: run:
```bash
./run_qualimap.sh
```

### Run MultiQC on Qualimap outputs
[`MultiQC`](https://github.com/MultiQC/MultiQC) 
<details>
<summary>ğŸ“Š Aggregating BAM QC with MultiQC</summary>

After running **Qualimap BAM QC**, each sample produces individual HTML and PDF reports. For large TB cohorts, opening these reports one by one is inefficient. **MultiQC** solves this by aggregating all results.

### Why we use MultiQC after Qualimap
- **Scans all Qualimap output folders**: Automatically detects per-sample QC reports.  
- **Aggregates QC metrics**: Combines mapping quality, depth, and coverage statistics across all samples.  
- **Quick overview of problematic samples**: Highlights low coverage, uneven depth, or poor alignment.  
- **Standardized reporting**: Ensures results are consistent, shareable, and suitable for team collaboration or publications.  

</details>

---

##### Step 1: **Open nano to create the script `run_multiqc_qualimap.sh`
```bash
nano run_multiqc_qualimap.sh
```
##### Step 2: Paste the following code into nano
```bash
#!/bin/bash
set -euo pipefail

INPUT_DIR="qualimap_reports"
OUTPUT_DIR="multiqc/qualimap_multiqc"

mkdir -p "$OUTPUT_DIR"

if [ ! -d "$INPUT_DIR" ]; then
    echo "Error: Input directory '$INPUT_DIR' does not exist!"
    exit 1
fi

multiqc "$INPUT_DIR" -o "$OUTPUT_DIR"

echo "MultiQC report generated in '$OUTPUT_DIR'."

```
<details>
<summary>ğŸ“‘ Generate MultiQC Report for Qualimap</summary>

Generates a MultiQC report from per-sample Qualimap BAM QC results.  

- ğŸ›¡ `set -euo pipefail` â†’ safe script execution  
- ğŸ“‚ `INPUT_DIR="qualimap_reports"` â†’ folder with Qualimap per-sample reports  
- ğŸ“‚ `OUTPUT_DIR="multiqc/qualimap_multiqc"` â†’ folder to store MultiQC report; auto-created  
- âš  Checks if input directory exists; exits if not  
- ğŸ”„ Runs `multiqc` on input folder and outputs to the specified directory  
- ğŸ–¨ Prints confirmation message with report location  

</details>

##### Step 3: Save & exit nano
Press CTRL+O, Enter (save)
Press CTRL+X (exit)
##### Step 4: Make the script executable
```bash
chmod +x run_multiqc_qualimap.sh
```
##### Step 5: Activate your conda env and run
```bash
conda activate multiqc_env
./run_multiqc_qualimap.sh
```

# 7ï¸âƒ£ High-Confidence Variant Filtering with **tb_variant_filter**
[`tb_variant_filter`](https://github.com/COMBAT-TB/tb_variant_filter) 
<details>
<summary>ğŸ§¬ TB-Specific Variant Filtering with <code>tb_variant_filter</code></summary>

The **tb_variant_filter** tool is designed specifically for **Mycobacterium tuberculosis (M. tb)** sequencing data. Unlike generic variant filters, it takes into account TB-specific genomic features and problematic regions in the **H37Rv reference genome**, ensuring that only **high-confidence variants** are retained.

### ğŸ›  Key Filtering Options

**tb_variant_filter** offers several ways to refine your VCF files:

1. **Region-based filtering**  
   Mask out variants in defined genomic regions. Region lists include:  
   - **RLC (Refined Low Confidence) regions** â€“ Marin et al 2022 (default)  
   - **RLC + Low Mappability regions** â€“ Marin et al 2022  
   - **PE/PPE genes** â€“ Fishbein et al 2015  
   - **Antibiotic resistance genes** â€“ TBProfiler and MTBseq lists  
   - **Repetitive loci** â€“ UVP list  

   > âš ï¸ **Default:** Use RLC regions. These are parts of the H37Rv genome where Illumina reads map poorly. For reads shorter than 100 bp or single-ended reads, consider using the **RLC + Low Mappability filter**. PE/PPE and UVP filters are mainly for backward compatibility, but they may exclude too much of the genome.

2. **Window around indels**  
   Masks variants within a set distance (default 5 bases) of insertions or deletions.

3. **Alternate allele percentage**  
   Removes variants with fewer than the minimum percentage (default 90%) of alternative alleles.

4. **Depth of aligned reads**  
   Filters variants based on sequencing depth to remove low-confidence calls.

5. **SNV-only filtering**  
   Optionally discard all variants that are not single nucleotide variants.

### âš¡ How Filters Work Together

When multiple filters are applied, they **stack**: a variant will be masked if **any filter** flags it. This ensures a conservative, high-confidence set of variants for downstream analyses.

</details>

---
## Steps

##### Step 1: Activate environment
```bash
conda activate tb_variant_filter_env
```
##### Step 2:  We need this directory to store all BED files containing genomic regions to be masked during variant filtering
```bash
mkdir -p ./masking_regions
```
#####  Step 3: Generate each BED file in that folder
<details>
<summary>ğŸ§¬ BED Files for TB Variant Filtering</summary>

We need **Refined Low Confidence (RLC) regions** by default, but we may also need other regions. You can download and store the BED files in your directory to mask genomic regions during variant filtering, such as:

- **Refined Low Confidence (RLC) regions**  
- **Low mappability regions**  
- **PE/PPE genes**  
- **Antibiotic resistance gene lists** (TBProfiler, MTBseq)  
- **Repetitive loci** (UVP)

</details>

```bash
tb_region_list_to_bed --chromosome_name H37Rv farhat_rlc ./masking_regions/RLC_Marin2022.bed
tb_region_list_to_bed --chromosome_name H37Rv farhat_rlc_lowmap ./masking_regions/RLC_and_LowMappability_Marin2022.bed
tb_region_list_to_bed --chromosome_name H37Rv pe_ppe ./masking_regions/PE_PPE_Fishbein2015.bed
tb_region_list_to_bed --chromosome_name H37Rv tbprofiler ./masking_regions/TBProfiler_resistance_genes.bed
tb_region_list_to_bed --chromosome_name H37Rv mtbseq ./masking_regions/MTBseq_resistance_genes.bed
tb_region_list_to_bed --chromosome_name H37Rv uvp ./masking_regions/UVP_repetitive_loci.bed

```
<details>
<summary>ğŸ§¬ tb_region_list_to_bed Commands Explanation</summary>

- `tb_region_list_to_bed` â†’ Command from `tb_variant_filter` to export predefined genomic regions as BED files.  
- `--chromosome_name H37Rv` â†’ Specifies the reference genome (M. tuberculosis H37Rv).  
- `{farhat_rlc, farhat_rlc_lowmap, pe_ppe, tbprofiler, mtbseq, uvp}` â†’ Region list names available in `tb_variant_filter`:  
  - `farhat_rlc` â†’ Refined Low Confidence regions (Marin et al., 2022).  
  - `farhat_rlc_lowmap` â†’ RLC + Low Mappability regions (Marin et al., 2022).  
  - `pe_ppe` â†’ PE/PPE gene regions (Fishbein et al., 2015).  
  - `tbprofiler` â†’ TBProfiler antibiotic resistance genes.  
  - `mtbseq` â†’ MTBseq antibiotic resistance genes.  
  - `uvp` â†’ UVP repetitive loci in the genome.  
- Last argument â†’ Output BED file path where the exported regions will be saved.  

</details>

##### Step 4: Create or edit the script  
```bash
nano run_tb_variant_filter.sh
```
#####  Step 5: Paste the following into `run_tb_variant_filter.sh`
```bash
#!/bin/bash
set -euo pipefail

CURDIR=$(pwd)
SNIPPY_DIR="$CURDIR/snippy_results"
OUTDIR="$CURDIR/tb_variant_filter_results"
mkdir -p "$OUTDIR"

REGION_FILTER="farhat_rlc"

for vcf in "$SNIPPY_DIR"/*.vcf; do
    sample=$(basename "$vcf")
    echo "Filtering $sample ..."
    
    tb_variant_filter --region_filter "$REGION_FILTER" \
        "$vcf" \
        "$OUTDIR/${sample%.vcf}.filtered.vcf"

        if [ ! -s "$OUTDIR/${sample%.vcf}.filtered.vcf" ]; then
        echo "âš  $sample filtered VCF is empty."
    fi
done

echo "âœ… All VCFs filtered using $REGION_FILTER and saved in $OUTDIR"

```
<details>
<summary>ğŸ§¬ TB Variant Filtering</summary>

Filters Snippy-generated VCF files using `tb_variant_filter` with a specified region filter.  

- ğŸ›¡ `set -euo pipefail` â†’ safe script execution  
- ğŸ“‚ `SNIPPY_DIR="snippy_results"` â†’ folder with Snippy VCF files  
- ğŸ“‚ `OUTDIR="tb_variant_filter_results"` â†’ output folder for filtered VCFs; auto-created  
- ğŸ· `REGION_FILTER="farhat_rlc"` â†’ region filter applied to variants  
- ğŸ”„ Loops through all `.vcf` files  
- âš¡ Runs `tb_variant_filter --region_filter` for each VCF and saves as `.filtered.vcf`  
- âš  Warns if filtered VCF is empty  
- âœ… Prints completion message with output directory  

</details>

##### Step 6: Save and exit nano
Press Ctrl + O â†’ Enter (to write the file)
Press Ctrl + X â†’ Exit nano

##### Step 7: Make the script executable
```bash
chmod +x run_tb_variant_filter.sh
```
##### Step 8: run
```bash
./run_tb_variant_filter.sh
```
<details>
<summary>ğŸ“Š VCF QC Script Step-by-Step Guide</summary>

### Purpose

This script is designed to compare unfiltered Snippy VCFs with filtered VCFs produced by `tb_variant_filter`. The main goals are:

- Count the total number of variants in each VCF.
- Identify variants that meet a custom â€œPASSâ€ criterion, based on user-defined quality thresholds.
- Calculate the PASS retention ratio, i.e., the fraction of high-quality variants retained after filtering.

This QC step ensures that your filtered VCFs retain high-confidence variants and helps detect potential over-filtering or loss of important variants before downstream analysis.

---

### Key Parameters

- `MIN_DP=20` â†’ Minimum read depth required for a variant to be considered â€œhigh quality.â€  
  - Read depth (DP in the VCF INFO field) indicates the number of reads supporting a variant.  
  - Variants with fewer than 20 supporting reads are considered low-confidence and are excluded from the PASS count.

- `MIN_QUAL=30` â†’ Minimum variant quality score (QUAL in the VCF) to consider a variant as PASS.  
  - The QUAL score represents the confidence that the variant is real.  
  - Variants with QUAL < 30 are considered low-confidence and are excluded from the PASS count.

> These thresholds can be adjusted depending on your experimental design and sequencing quality.

---

### What the Script Does
For each sample:
**Unfiltered VCF analysis**
- Counts all variants (`Unfiltered_total`).
- Counts variants meeting `DP â‰¥ 20` and `QUAL â‰¥ 30` (`Unfiltered_PASS`).
**Filtered VCF analysis**
- Counts all variants in the filtered VCF (`Filtered_total`).
- Counts variants meeting `DP â‰¥ 20` and `QUAL â‰¥ 30` (`Filtered_PASS`).
**Calculate PASS retention ratio**
- `PASS_retention_ratio = Filtered_PASS / Unfiltered_PASS`  
- Measures how many high-quality variants remain after filtering.
---

### Output Table Columns

| Column               | Description                                                                 |
|----------------------|-----------------------------------------------------------------------------|
| Sample               | Name of the sample (derived from the VCF filename)                           |
| Unfiltered_total     | Total variants in the original Snippy VCF                                    |
| Unfiltered_PASS      | Variants meeting `DP â‰¥ 20` and `QUAL â‰¥ 30` in unfiltered VCF                |
| Filtered_total       | Total variants in tb_variant_filter output                                   |
| Filtered_PASS        | Variants meeting `DP â‰¥ 20` and `QUAL â‰¥ 30` in filtered VCF                  |
| PASS_retention_ratio | Fraction of high-quality variants retained after filtering                  |

</details>

##### Step 1: Open a new file in nano
```bash
nano compare_vcf_qc.sh
```
##### Step 2: Paste the script
```bash
#!/bin/bash
set -euo pipefail

SNIPPY_DIR="snippy_results"
FILTERED_DIR="tb_variant_filter_results"
OUTDIR="csv_output"
OUTFILE="$OUTDIR/variant_filter_summary.csv"

MIN_DP=20
MIN_QUAL=30

mkdir -p "$OUTDIR"
echo "Sample,Unfiltered_total,Unfiltered_PASS,Filtered_total,Filtered_PASS,PASS_retention_ratio" | tee "$OUTFILE"

count_pass_variants() {
    local vcf=$1
    awk -v min_dp=$MIN_DP -v min_qual=$MIN_QUAL '
        BEGIN{count=0}
        /^#/ {next}
        {
            qual=$6
            dp=0
            split($8, info_arr, ";")
            for(i in info_arr){
                if(info_arr[i] ~ /^DP=/){
                    split(info_arr[i], kv, "=")
                    dp=kv[2]
                }
            }
            if(qual >= min_qual && dp >= min_dp) count++
        }
        END{print count}
    ' "$vcf"
}

for vcf in "$SNIPPY_DIR"/*.vcf; do
    sample=$(basename "$vcf" .vcf)

    # safety: skip if VCF is empty
    if [[ ! -s "$vcf" ]]; then
        echo "$sample,NA,NA,NA,NA,NA" | tee -a "$OUTFILE"
        continue
    fi

    unfiltered_total=$(grep -v "^#" "$vcf" | wc -l)
    unfiltered_pass=$(count_pass_variants "$vcf")

    if [[ ! -f "$FILTERED_DIR/$sample.filtered.vcf" ]]; then
        echo "$sample,$unfiltered_total,$unfiltered_pass,NA,NA,NA" | tee -a "$OUTFILE"
        continue
    fi

    filtered_total=$(grep -v "^#" "$FILTERED_DIR/$sample.filtered.vcf" | wc -l)
    filtered_pass=$(count_pass_variants "$FILTERED_DIR/$sample.filtered.vcf")

    if [[ $unfiltered_pass -eq 0 ]]; then
        ratio="NA"
    else
        ratio=$(awk -v pass=$filtered_pass -v total=$unfiltered_pass 'BEGIN{printf "%.2f", pass/total}')
    fi

    echo "$sample,$unfiltered_total,$unfiltered_pass,$filtered_total,$filtered_pass,$ratio" | tee -a "$OUTFILE"
done
```
<details>
<summary>ğŸ“Š Variant Filter Summary</summary>

Generates a CSV summary comparing unfiltered and filtered TB VCF variants.  

- ğŸ›¡ `set -euo pipefail` â†’ safe script execution  
- ğŸ“‚ Input directories:  
  - `SNIPPY_DIR="snippy_results"` â†’ unfiltered VCFs  
  - `FILTERED_DIR="tb_variant_filter_results"` â†’ filtered VCFs  
- ğŸ“‚ `OUTDIR="csv_output"` â†’ stores summary CSV (`variant_filter_summary.csv`)  
- âš¡ Filters variants based on:  
  - Minimum depth: `MIN_DP=20`  
  - Minimum quality: `MIN_QUAL=30`  
- ğŸ§® `count_pass_variants()` â†’ counts variants meeting depth & quality thresholds  
- ğŸ”„ Loops through samples to calculate:  
  - Total variants (unfiltered/filtered)  
  - PASS variants (unfiltered/filtered)  
  - PASS retention ratio  
- ğŸ–¨ Appends results to CSV and prints per sample  

</details>

##### Step  3: Save and exit nano
   Press Ctrl+O â†’ Enter to save.
   Press Ctrl+X â†’ Exit nano.
##### Step  4:Make the script executable
```bash
chmod +x compare_vcf_qc.sh
```
##### Step 5: Run the script
```bash
./compare_vcf_qc.sh
```

# 8ï¸âƒ£ Consensus Genome Generation with **BCFtools / SAMtools** & Outgroup Inclusion
[`BCFtools`](https://github.com/samtools/bcftools) / [`SAMtools`](https://github.com/samtools/samtools)
<details>
<summary>ğŸ§¬ Generate Sample-Specific Consensus Sequences</summary>

After filtering VCFs with **tb_variant_filter**, we generate **consensus FASTA sequences** for each sample. These sequences represent the **full genome of each isolate**, including only **high-confidence variants** relative to the reference genome (*H37Rv*).

### Why consensus sequences matter
- Provide a **single representative genome** per sample for downstream analyses.  
- Used in **phylogenetic reconstruction**, outbreak investigation, and comparative genomics.  
- Incorporate **only reliable SNPs and indels**, minimizing noise from sequencing errors.  
- Standardize genome representations across multiple isolates, ensuring comparability.  

</details>
---

##### Step 1: Compress and index each filtered VCF
```bash
for vcf in tb_variant_filter_results/*.vcf; do
   
    if [ ! -s "$vcf" ]; then
        echo "Skipping empty VCF: $vcf"
        continue
    fi

    gz_file="${vcf}.gz"
    echo "Compressing $vcf â†’ $gz_file"
    bgzip -c "$vcf" > "$gz_file"

    echo "Indexing $gz_file"
    bcftools index "$gz_file"
done
```
<details>
<summary>ğŸ—„ï¸ Compress & Index VCFs</summary>

Compresses and indexes filtered VCF files for downstream analysis.  

- ğŸ”„ Loops through all `.vcf` files in `tb_variant_filter_results/`  
- âš  Skips empty VCF files  
- ğŸ§© Compresses VCFs using `bgzip` â†’ creates `.vcf.gz`  
- ğŸ“‚ Indexes compressed VCFs with `bcftools index`  
- ğŸ–¨ Prints progress messages for each file  

</details>

##### Step 2: Create a script to generate consensus sequences
```bash
nano generate_consensus_all.sh
```
##### Step 3: Paste this code
```bash
#!/bin/bash
set -euo pipefail

CURDIR=$(pwd)
VCFDIR="$CURDIR/tb_variant_filter_results"
OUTDIR="$CURDIR/consensus_sequences"
mkdir -p "$OUTDIR"

for vcf in "$VCFDIR"/*.vcf; do
    sample=$(basename "$vcf" .vcf)

    if [ $(grep -v '^#' "$vcf" | wc -l) -eq 0 ]; then
        echo "$sample VCF is empty. Skipping."
        continue
    fi

    gz_file="${vcf}.gz"
    bgzip -c "$vcf" > "$gz_file"
    bcftools index "$gz_file"
    bcftools consensus -f "$CURDIR/H37Rv.fasta" "$gz_file" | sed "1s/.*/>$sample/" > "$OUTDIR/${sample}.consensus.fasta"
done
```
<details>
<summary>ğŸ§¬ Generate Consensus Sequences</summary>

Generates per-sample consensus FASTA sequences from filtered VCFs.  

- ğŸ›¡ `set -euo pipefail` â†’ safe script execution  
- ğŸ“‚ `VCFDIR="tb_variant_filter_results"` â†’ folder with filtered VCFs  
- ğŸ“‚ `OUTDIR="consensus_sequences"` â†’ stores consensus FASTA files; auto-created  
- ğŸ”„ Loops through all `.vcf` files  
- âš  Skips empty VCFs  
- ğŸ—„ Compresses VCFs using `bgzip` and indexes with `bcftools index`  
- ğŸ§© Generates consensus sequence using `bcftools consensus` and reference `H37Rv.fasta`  
- ğŸ· Replaces FASTA header with sample name and saves output  
- ğŸ–¨ Prints progress messages for each sample  

</details>


##### Step 4: Save and exit nano
Press Ctrl + O â†’ Enter (to write the file)
Press Ctrl + X â†’ Exit nano

##### Step 5: Make the script executable
```bash
chmod +x generate_consensus_all.sh
```
##### Step 6: Run the script
```bash
conda activate tb_consensus_env
./generate_consensus_all.sh
```
Check Consensus FASTA Lengths

After generating consensus sequences, it's important to **verify the genome length** for each sample.  
This ensures no sequences are truncated or incomplete due to missing coverage or filtering.

---

### ğŸ“ Calculating Consensus Genome Lengths

We can check the length of each consensus FASTA sequence to ensure completeness and consistency.  
This helps verify that consensus sequences cover the full *M. tuberculosis* genome (~4.4 Mbp) and can reveal missing regions.

### Rename the FASTA files

```bash
#!/bin/bash
FASTA_DIR="consensus_sequences"

for f in "$FASTA_DIR"/*.filtered.consensus.fasta; do
    mv "$f" "${f/.filtered.consensus/}"
done

echo "âœ… All consensus FASTA files have been renamed to .fasta."

```

<details>
<summary>âœï¸ Rename Consensus FASTA Files</summary>

Simplifies filenames of consensus sequences by removing `.filtered.consensus` from the name.  

- ğŸ“‚ `FASTA_DIR="consensus_sequences"` â†’ folder with consensus FASTA files  
- ğŸ”„ Loops through all `.filtered.consensus.fasta` files  
- âœ‚ Renames files to remove `.filtered.consensus` â†’ results in `.fasta`  
- ğŸ–¨ Prints confirmation message after renaming  

</details>


###  Update headers inside the FASTA files
```bash
#!/bin/bash
FASTA_DIR="consensus_sequences"

for f in "$FASTA_DIR"/*.fasta; do
    sample=$(basename "$f" .fasta)
    awk -v s="$sample" '/^>/{print ">" s; next} {print}' "$f" > "${f}.tmp" && mv "${f}.tmp" "$f"
    echo "âœ… Updated header in: $(basename "$f")"
done
echo "ğŸ‰ All FASTA headers have been successfully updated."
```
<details>
<summary>ğŸ“ Update FASTA Headers</summary>

Updates FASTA headers so each sequence header matches its sample name.  

- ğŸ“‚ `FASTA_DIR="consensus_sequences"` â†’ folder with consensus FASTA files  
- ğŸ”„ Loops through all `.fasta` files  
- âœ‚ Replaces header (`>`) with the sample name extracted from the filename  
- ğŸ—„ Saves changes by overwriting original files  
- ğŸ–¨ Prints confirmation for each file and a final success message  

</details>

### Using `grep` and `wc`
We remove the FASTA headers (`>` lines) and count the remaining nucleotides to get the total genome length:

```bash
for f in consensus_sequences/*.fasta; do
    sample=$(basename "$f")
    length=$(grep -v ">" "$f" | tr -d '\n' | wc -c)
    echo "$sample : $length bp"
done
```
<details>
<summary>ğŸ“ Report Consensus Sequence Lengths</summary>

Calculates and prints the length (in base pairs) of each consensus FASTA file.  

- ğŸ“‚ `consensus_sequences/*.fasta` â†’ folder with consensus FASTA files  
- ğŸ”„ Loops through all FASTA files  
- âœ‚ Strips headers (`>`) and concatenates sequence lines  
- ğŸ§® Counts total bases using `wc -c`  
- ğŸ–¨ Prints sample name and sequence length in bp  

</details>

to save the result in csv file 
```bash
#!/bin/bash
FASTA_DIR="consensus_sequences"
OUTDIR="csv_output"
mkdir -p "$OUTDIR"  
OUTPUT_CSV="${OUTDIR}/consensus_lengths.csv"

echo "Sample,Length_bp" > "$OUTPUT_CSV"

for f in "$FASTA_DIR"/*.fasta; do
    sample=$(basename "$f" .fasta)
    length=$(grep -v ">" "$f" | tr -d '\n' | wc -c)
    echo "$sample,$length" >> "$OUTPUT_CSV"
done

echo "âœ… Consensus genome lengths saved to $OUTPUT_CSV"

```
<details>
<summary>ğŸ§® Export Consensus Genome Lengths</summary>

Calculates the length of each consensus FASTA sequence and saves results to a CSV.  

- ğŸ“‚ `FASTA_DIR="consensus_sequences"` â†’ folder with consensus FASTA files  
- ğŸ“‚ `OUTDIR="csv_output"` â†’ folder for CSV output; auto-created  
- ğŸ—„ `OUTPUT_CSV="csv_output/consensus_lengths.csv"` â†’ output CSV file  
- ğŸ”„ Loops through all FASTA files  
- âœ‚ Strips headers (`>`) and counts bases  
- ğŸ–¨ Appends sample name and length (bp) to CSV  
- âœ… Prints confirmation message with CSV location  

</details>


# 9ï¸âƒ£ Multiple Sequence Alignment

[`MAFFT`](https://github.com/GSLBiotech/mafft) requires **a single FASTA file** as input.  
It **cannot take multiple FASTA files** on the command line directly, otherwise it interprets filenames as options.

---

Before performing phylogenetic analysis, it is crucial to include an **outgroup sequence**. The outgroup serves as a reference point to root the phylogenetic tree, providing directionality for evolutionary relationships.  

For our analysis, we searched the NCBI database for sequences belonging to **Lineage 8** of *Mycobacterium tuberculosis*. We selected the sequence `SRR10828835`, which originates from Rwanda, as our outgroup. This choice was intentional because Lineage 8 is **not present in Ethiopia**, ensuring it is sufficiently divergent from our study isolates and providing a robust root for the tree.  

The selected outgroup sequence was downloaded and saved in the `consensus_sequences/` directory alongside our aligned TB consensus sequences.

---
Perform multiple sequence alignment on the merged file:
Use a faster algorithm than --auto

--auto lets MAFFT decide, but for hundreds/thousands of sequences, it often picks a slower iterative refinement.
You can explicitly pick faster modes:

Since Mycobacterium tuberculosis genomes are highly conserved and we care more about speed than very small accuracy gains, we can safely drop the expensive iterative refinement steps in MAFFT.
fast and TB-suitable command

##### Step 1: Merge all consensus FASTAs
We combine all individual consensus sequences into one multi-FASTA file:
- **Aligning sequences**  
  - All sequences, including the outgroup, must be aligned together.  
  - This ensures that **homologous positions are matched across all sequences**, which is essential for correct tree inference.  
  - Example workflow:  
    1. Combine all consensus sequences and the outgroup FASTA into a single folder.  
    2. Run MAFFT on the combined sequences to produce a single aligned file (e.g., `aligned_consensus.fasta`).  
  - The outgroup sequence must already be included in the alignment.  
  - When running IQ-TREE, we need to specify the outgroup using the `-o` option with the **FASTA header name** of the outgroup:  
  - IQ-TREE will then **root the phylogenetic tree using this outgroup**.
    
```bash
cat consensus_sequences/*.fasta > consensus_sequences/all_consensus.fasta
```

##### Step 2: Paste the script into nano
```bash
#!/bin/bash
mkdir -p mafft_results
mafft --auto --parttree consensus_sequences/all_consensus.fasta > mafft_results/aligned_consensus.fasta
```
<details>
<summary>ğŸ§¬ Align Consensus Sequences with MAFFT</summary>

Performs multiple sequence alignment of consensus genomes using MAFFT.  

- ğŸ“‚ `consensus_sequences/all_consensus.fasta` â†’ input FASTA with all consensus sequences  
- ğŸ“‚ `mafft_results` â†’ output folder for aligned sequences; auto-created  
- âš¡ Runs `mafft --auto --parttree` for optimized alignment  
- ğŸ—„ Saves aligned sequences to `mafft_results/aligned_consensus.fasta`  

</details>

##### Step 3: Verify the alignment

A. Quickly inspect the top of the aligned FASTA:
```bash
head mafft_results/aligned_consensus.fast
```
B. Quick visual inspection with `less`
```bash
fold -w 100 mafft_results/aligned_consensus.fasta | less
```
C. Checking MAFFT alignment output to 
- Ensure the alignment file **exists** and contains all intended sequences.  
- Verify the **number of sequences** matches expectations.  
- Check **sequence lengths** (total, min, max, average) to confirm proper alignment.  
- Detect if sequences have **varying lengths**, which may indicate misalignment or excessive gaps.  
- A **good alignment** is essential for accurate phylogenetic tree inference with IQ-TREE or other downstream analyses.

```bash
#!/bin/bash
set -euo pipefail

FILE="mafft_results/aligned_consensus.fasta"

if [[ ! -f "$FILE" ]]; then
    echo "âŒ $FILE not found!"
    exit 1
fi

SEQ_COUNT=$(grep -c ">" "$FILE")
LENGTHS=($(grep -v ">" "$FILE" | awk 'BEGIN{RS=">"} NR>1{print length($0)}'))
TOTAL_LENGTH=$(IFS=+; echo "$((${LENGTHS[*]}))")
MIN_LENGTH=$(printf "%s\n" "${LENGTHS[@]}" | sort -n | head -n1)
MAX_LENGTH=$(printf "%s\n" "${LENGTHS[@]}" | sort -n | tail -n1)
AVG_LENGTH=$(( TOTAL_LENGTH / SEQ_COUNT ))

echo "âœ… $FILE exists"
echo "Sequences: $SEQ_COUNT"
echo "Total length (bp): $TOTAL_LENGTH"
echo "Min length: $MIN_LENGTH"
echo "Max length: $MAX_LENGTH"
echo "Avg length: $AVG_LENGTH"

if [[ $MIN_LENGTH -eq $MAX_LENGTH ]]; then
    echo "âœ… All sequences have equal length, alignment looks good."
else
    echo "âš ï¸ Sequence lengths vary; check for gaps or misalignment."
fi

```
<details>
<summary>ğŸ” Check MAFFT Alignment Quality</summary>

Performs basic QC on the aligned consensus sequences to assess length consistency.  

- ğŸ“‚ `FILE="mafft_results/aligned_consensus.fasta"` â†’ input aligned FASTA  
- âš  Exits if file does not exist  
- ğŸ§® Counts sequences, total bases, minimum, maximum, and average sequence lengths  
- ğŸ–¨ Prints summary stats per alignment  
- âœ… Checks if all sequences have equal length; flags potential misalignment if lengths vary  

</details>

D. Using `seqkit` stats (recommended)
seqkit is a fast toolkit for FASTA/Q file summaries. It gives a detailed report of sequences in a file:
```bash
conda activate seqkit_env
seqkit stats mafft_results/aligned_consensus.fasta
```
E. Check for gaps / alignment columns

<details>
<summary>ğŸ’¡ Understanding Gaps in Multiple Sequence Alignment (MSA)</summary>

When you do a **multiple sequence alignment (MSA):**

- All sequences should line up **column by column**.
- **MAFFT** will add **gaps (`-`)** where needed so that homologous positions align.
- A good alignment usually has **all sequences the same length**, including gaps.
- If sequence lengths differ, it often indicates:
  - Truncated sequences
  - Missing data
  - Alignment problems

So, checking the **length of each sequence** in the alignment is a quick way to see if **gaps and sequences are consistent**.

</details>

```bash
grep -v ">" mafft_results/aligned_consensus.fasta \
| awk '{print length($0)}' \
| sort -n \
| uniq -c \
| awk -v L=60 '{print ($2<L?"âš ï¸ ":"âœ… ") $1 " sequences of length " $2 " bp"}'
```
<details>
<summary>ğŸ” Line-by-Line Explanation of Alignment Check Command</summary>

- `grep -v ">" mafft_results/aligned_consensus.fasta` â†’ removes FASTA headers, leaving only sequence lines.  
- `awk '{print length($0)}'` â†’ prints the length of each sequence line.  
- `sort -n` â†’ sorts sequence lengths numerically.  
- `uniq -c` â†’ counts how many sequences have each unique length.  
- `awk -v L=60 '{print ($2<L?"âš ï¸ ":"âœ… ") $1 " sequences of length " $2 " bp"}'` â†’ formats output: âš ï¸ for lengths <60, âœ… for â‰¥60, showing count and length.

</details>

F. Compute pairwise identity
```bash
awk '/^>/{if(seqlen){print seqlen}; seqlen=0; next} {seqlen+=length($0)} END{print seqlen}' mafft_results/aligned_consensus.fasta \
| sort -n \
| uniq -c \
| awk -v L=60 '{print ($2<L?"âš ï¸ ":"âœ… ") $1 " sequences of length " $2 " bp"}'
```
<details>
<summary>ğŸ“ Aligned Sequence Length Summary</summary>

Summarizes the length distribution of sequences in an aligned FASTA.  

- ğŸ“‚ `mafft_results/aligned_consensus.fasta` â†’ input aligned FASTA  
- ğŸ”„ Computes the length of each sequence  
- ğŸ“Š Counts unique lengths and number of sequences per length  
- âš  Marks sequences shorter than 60 bp with a warning  
- âœ… Prints summary with sequence counts and lengths  

</details>

G. Use AMAS (Alignment Manipulation and Summary)
AMAS is a Python tool to summarize alignments:
```bash
conda activate amas_env
AMAS.py summary -f fasta -d dna -i mafft_results/aligned_consensus.fasta
```
<details>
<summary>ğŸ“Š AMAS Alignment Summary</summary>

Generates a summary of the aligned consensus sequences using AMAS.  

- ğŸ `conda activate amas_env` â†’ activates the AMAS environment  
- âš¡ `AMAS.py summary` â†’ summarizes alignment statistics  
- ğŸ”¹ Options:  
  - `-f fasta` â†’ input format is FASTA  
  - `-d dna` â†’ sequence type is DNA  
  - `-i mafft_results/aligned_consensus.fasta` â†’ input aligned FASTA file  
- ğŸ–¨ Outputs alignment summary including sequence counts, lengths, and base composition  

</details>

H. Use aliview or MEGA for GUI inspection
Load the FASTA alignment in AliView, MEGA, or Geneious.
Advantages:
  Can visually check gaps, conserved regions, and misaligned sequences.
  Highlight sequences that differ significantly.


# ğŸ”Ÿ Phylogenetic Tree Construction with **IQ-TREE**
[`IQ-TREE`](https://github.com/iqtree/iqtree2) 
<details>
<summary>ğŸ“– Overview of IQ-TREE and important concepts</summary>

- **What is IQ-TREE?**  
  - IQ-TREE is a **phylogenetic tree inference software** used to construct evolutionary trees from aligned sequences.  
  - It implements **maximum likelihood (ML) methods**, which estimate the tree that best explains the observed sequences under a substitution model.  

- **Input**  
  - Requires an **aligned sequence file** (FASTA, PHYLIP, or NEXUS).  
  - Sequences must be aligned so that homologous positions are in the same columns.  

- **Substitution models (-m)**  
  - Models describe how nucleotides (or amino acids) change over time.  
  - Common nucleotide models:  
    - **GTR**: General Time Reversible model, allows different rates for each nucleotide substitution.  
    - **+G**: Gamma distribution to account for rate variation across sites.  
    - **+I**: Proportion of invariant sites.  
  - IQ-TREE can **automatically select the best model** using `-m MFP` (ModelFinder Plus).  

- **Bootstrap support (-bb)**  
  - Ultrafast bootstrap replicates measure **confidence in the tree branches**.  
  - Example: `-bb 1000` runs 1000 replicates.  

- **Outgroup rooting (-o)**  
  - Specify the outgroup sequence (by its FASTA header) to **root the tree** correctly.  
  - The outgroup must be included in the alignment.  

- **Multithreading (-nt)**  
  - IQ-TREE can use multiple CPU threads for faster computation, e.g., `-nt 4`.  

- **Output files**  
  - `.treefile` â†’ final tree in Newick format.  
  - `.log` â†’ details of the run.  
  - `.iqtree` â†’ summary of models, likelihoods, and bootstrap values.  

- **Key points**  
  - Input sequences must be properly aligned.  
  - The choice of substitution model affects tree accuracy.  
  - Use the outgroup for rooting but include it in the alignment.  
  - Bootstrapping provides branch support values for interpreting the tree.  

</details>

 Steps 
##### Step 1: activate iqtree environment
```bash
conda activate iqtree_env
```
##### Step 2: Run the following script
```bash
mkdir -p iqtree_results

iqtree2 -s mafft_results/aligned_consensus.fasta \
        -m GTR+G \
        -bb 1000 \
        -nt 4 \
        -o SRR10828835 \
        -pre iqtree_results/aligned_consensus
```
<details>
<summary>ğŸŒ³ Build Phylogenetic Tree with IQ-TREE</summary>

Constructs a maximum likelihood phylogenetic tree from aligned consensus sequences.  

- ğŸ“‚ `mafft_results/aligned_consensus.fasta` â†’ input aligned FASTA  
- ğŸ“‚ `iqtree_results` â†’ output folder; auto-created  
- âš¡ `iqtree2` options:  
  - `-m GTR+G` â†’ substitution model (GTR with gamma rate heterogeneity)  
  - `-bb 1000` â†’ 1000 ultrafast bootstrap replicates  
  - `-nt 4` â†’ use 4 threads  
  - `-o SRR10828835` â†’ designate outgroup  
  - `-pre iqtree_results/aligned_consensus` â†’ prefix for output files  
- ğŸ–¨ Generates tree files and bootstrap support values  

</details>


### ğŸŒ³ Visualization with TB-Profiler + iTOL

TB-Profiler outputs can be integrated into [iTOL](https://itol.embl.de/) for rich phylogenetic visualization, combining resistance, lineage, and metadata.

* **Drug resistance types**:  
  - **HR-TB**: Isoniazid-resistant TB  
  - **RR-TB**: Rifampicin-resistant TB  
  - **MDR-TB**: Multidrug-resistant TB (at least INH + RIF resistant)  
  - **XDR-TB**: Extensively drug-resistant TB (MDR + fluoroquinolone + second-line injectable resistant)  
  - **Other**: Cases that do not fit the above categories  

  These categories can be represented in iTOL as colored rings around the phylogeny for quick classification.  

* **Individual drug resistance profiles**:  
  Each anti-TB drug is represented as a binary dot in iTOL:  
  - **Black dot** = Resistant  
  - **White dot** = Susceptible  

  This scheme provides a clear and compact way to compare resistance across isolates without cluttering the tree.  

* **Lineages & Sublineages**:  
  - Major M. tuberculosis lineages (1â€“10) can be highlighted with branch colors or background shading.  
  - Sublineages (e.g. 4.2.2.2, 2.2.1) can be shown as labels or an extra ring.  

# ğŸ“– References

1. WHO. *Catalogue of mutations in MTBC and their association with drug resistance*, 2nd ed, 2023.  

2. Chen, S., Zhou, Y., Chen, Y., & Gu, J. (2018). [*fastp: an ultra-fast all-in-one FASTQ preprocessor*](https://doi.org/10.1093/bioinformatics/bty560). **Bioinformatics**, 34(17), i884â€“i890.  

3. Ewels, P., Magnusson, M., Lundin, S., & KÃ¤ller, M. (2016). [*MultiQC: summarize analysis results for multiple tools and samples in a single report*](https://doi.org/10.1093/bioinformatics/btw354). **Bioinformatics**, 32(19), 3047â€“3048.  

4. Coll, F., McNerney, R., Preston, M. D., Guerra-AssunÃ§Ã£o, J. A., Warry, A., Hill-Cawthorne, G., â€¦ Clark, T. G. (2015). [*Rapid determination of anti-tuberculosis drug resistance from whole-genome sequences*](https://doi.org/10.1186/s13073-015-0164-0). **Genome Medicine**, 7, 51.  

5. Seemann, T. (2015). *Snippy: rapid haploid variant calling and core genome alignment*. Available at: [https://github.com/tseemann/snippy](https://github.com/tseemann/snippy)  

6. Okonechnikov, K., Conesa, A., & GarcÃ­a-Alcalde, F. (2016). [*Qualimap 2: advanced multi-sample quality control for high-throughput sequencing data*](https://doi.org/10.1093/bioinformatics/btv566). **Bioinformatics**, 32(2), 292â€“294.  

7. Walker, T. M., Merker, M., Knoblauch, A. M., Helbling, P., Schoch, O. D., van der Werf, T. S., â€¦ Niemann, S. (2022). *A cluster of multidrug-resistant Mycobacterium tuberculosis among patients arriving in Europe from the Horn of Africa: a molecular epidemiological study*. **Lancet Microbe**, 3(9), e672â€“e681. (Supplementary methods describe tb_variant_filter). Available at: [https://github.com/iqbal-lab-org/tb_variant_filter](https://github.com/iqbal-lab-org/tb_variant_filter)  

8. Danecek, P., Bonfield, J. K., Liddle, J., Marshall, J., Ohan, V., Pollard, M. O., â€¦ Li, H. (2021). [*Twelve years of SAMtools and BCFtools*](https://doi.org/10.1093/gigascience/giab008). **GigaScience**, 10(2), giab008.  

9. Katoh, K., & Standley, D. M. (2013). [*MAFFT multiple sequence alignment software version 7: improvements in performance and usability*](https://doi.org/10.1093/molbev/mst010). **Molecular Biology and Evolution**, 30(4), 772â€“780.  

10. Minh, B. Q., Schmidt, H. A., Chernomor, O., Schrempf, D., Woodhams, M. D., von Haeseler, A., & Lanfear, R. (2020). [*IQ-TREE 2: new models and efficient methods for phylogenetic inference in the genomic era*](https://doi.org/10.1093/molbev/msaa015). **Molecular Biology and Evolution**, 37(5), 1530â€“1534.  

11. Letunic, I., & Bork, P. (2021). [*Interactive Tree Of Life (iTOL) v5: an online tool for phylogenetic tree display and annotation*](https://doi.org/10.1093/nar/gkab301). **Nucleic Acids Research**, 49(W1), W293â€“W296.  

